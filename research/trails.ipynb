{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bef9096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/subhasis/Projects/med-chat/research'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6fc70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3812b09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/subhasis/Projects/med-chat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "283f10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "27b1674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from pdf file\n",
    "def load_pdf_files(data):\n",
    "    loader= DirectoryLoader(\n",
    "        path=data,glob=\"*.pdf\",loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5648d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data = load_pdf_files(\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c69753fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extract_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0f982cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs: List[Document]) -> list[Document]:\n",
    "    minimal_docs: List[Document] = []\n",
    "\n",
    "    for doc in docs:\n",
    "        src=doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": src}\n",
    "            )\n",
    "        )\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4bfd9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_doc = filter_to_minimal_docs(extract_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6c2c78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the docs in smaller chunks\n",
    "\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "         chunk_size=500,\n",
    "         chunk_overlap=20\n",
    "    )\n",
    "    texts_chunk = text_splitter.split_documents(minimal_docs)\n",
    "    return texts_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e2bcc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_chunk = text_split(minimal_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b93072bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 32 chunks\n",
      "âœ… Added batch of 3 chunks\n",
      "âœ… Added documents to Chroma\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import uuid\n",
    "\n",
    "# Suppose `docs` is a List[Document] from LangChain\n",
    "doc_texts = [doc.page_content for doc in texts_chunk]\n",
    "\n",
    "# 1ï¸âƒ£ Create embedding function\n",
    "embedding_fn = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"batch_size\": 32}\n",
    "    )\n",
    "\n",
    "# 2ï¸âƒ£ Generate actual embeddings\n",
    "embeddings = embedding_fn.embed_documents(doc_texts)  # âœ… This returns List[List[float]]\n",
    "\n",
    "# 3ï¸âƒ£ Create Chroma client\n",
    "client = chromadb.PersistentClient(path=\"../chroma_data\")\n",
    "collection_name = \"med\"\n",
    "\n",
    "# 4ï¸âƒ£ Get or create collection\n",
    "existing_collections = [col.name for col in client.list_collections()]\n",
    "if collection_name in existing_collections:\n",
    "    collection = client.get_collection(collection_name)\n",
    "else:\n",
    "    collection = client.create_collection(name=collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "# === 4. Helper: Batch Processing ===\n",
    "def batch_list(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "# === 5. Embed & Insert Incrementally ===\n",
    "for batch in batch_list(doc_texts, 32):\n",
    "    ids = [str(uuid.uuid4()) for _ in batch]\n",
    "    batch_embeddings = embedding_fn.embed_documents(batch)\n",
    "\n",
    "# 6ï¸âƒ£ Add documents & embeddings\n",
    "    collection.add(\n",
    "            ids=ids,\n",
    "            documents=batch,\n",
    "            embeddings=batch_embeddings\n",
    "        )\n",
    "    print(f\"âœ… Added batch of {len(batch)} chunks\")\n",
    "\n",
    "print(\"âœ… Added documents to Chroma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5a1020c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Query: abuse\n",
      "\n",
      "1. Score: 0.3593 | Text: Abuse\n",
      "Definition\n",
      "Abuse is defined as any thing that is harmful, injuri-\n",
      "ous, or offensive. Abuse als...\n",
      "2. Score: 0.3593 | Text: Abuse\n",
      "Definition\n",
      "Abuse is defined as any thing that is harmful, injuri-\n",
      "ous, or offensive. Abuse als...\n",
      "3. Score: 0.3593 | Text: Abuse\n",
      "Definition\n",
      "Abuse is defined as any thing that is harmful, injuri-\n",
      "ous, or offensive. Abuse als...\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# === CONFIG ===\n",
    "CHROMA_PATH = \"../chroma_data\"\n",
    "COLLECTION_NAME = \"med\"\n",
    "QUERY_TEXT = \"abuse\"  # Your search query\n",
    "TOP_K = 3  # Number of results to return\n",
    "\n",
    "# === 1. Load embedding function (must match what you used during insert) ===\n",
    "embedding_fn = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"batch_size\": 32}\n",
    "    )\n",
    "\n",
    "# === 2. Connect to Chroma and get collection ===\n",
    "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "# === 3. Embed the query ===\n",
    "query_embedding = embedding_fn.embed_query(QUERY_TEXT)\n",
    "\n",
    "# === 4. Search in Chroma ===\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"documents\", \"distances\"]\n",
    ")\n",
    "\n",
    "# === 5. Show results ===\n",
    "print(f\"\\nðŸ” Query: {QUERY_TEXT}\\n\")\n",
    "for i, (doc, distance) in enumerate(zip(results[\"documents\"][0], results[\"distances\"][0]), start=1):\n",
    "    print(f\"{i}. Score: {distance:.4f} | Text: {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555067d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Collection [med] does not exists",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[112]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     15\u001b[39m embedding_fn = HuggingFaceEmbeddings(\n\u001b[32m     16\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     encode_kwargs={\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m32\u001b[39m}\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m client = chromadb.PersistentClient(path=CHROMA_PATH)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m collection = client.get_collection(COLLECTION_NAME)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ==== 2. Get relevant documents ====\u001b[39;00m\n\u001b[32m     24\u001b[39m query_embedding = embedding_fn.embed_query(QUERY_TEXT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medbot/lib/python3.11/site-packages/chromadb/api/client.py:192\u001b[39m, in \u001b[36mClient.get_collection\u001b[39m\u001b[34m(self, name, embedding_function, data_loader)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(\n\u001b[32m    185\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    190\u001b[39m     data_loader: Optional[DataLoader[Loadable]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    191\u001b[39m ) -> Collection:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._server.get_collection(\n\u001b[32m    193\u001b[39m         name=name,\n\u001b[32m    194\u001b[39m         tenant=\u001b[38;5;28mself\u001b[39m.tenant,\n\u001b[32m    195\u001b[39m         database=\u001b[38;5;28mself\u001b[39m.database,\n\u001b[32m    196\u001b[39m     )\n\u001b[32m    197\u001b[39m     persisted_ef_config = model.configuration_json.get(\u001b[33m\"\u001b[39m\u001b[33membedding_function\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    199\u001b[39m     validate_embedding_function_conflict_on_get(\n\u001b[32m    200\u001b[39m         embedding_function, persisted_ef_config\n\u001b[32m    201\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medbot/lib/python3.11/site-packages/chromadb/api/rust.py:248\u001b[39m, in \u001b[36mRustBindingsAPI.get_collection\u001b[39m\u001b[34m(self, name, tenant, database)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(\n\u001b[32m    243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    246\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    247\u001b[39m ) -> CollectionModel:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     collection = \u001b[38;5;28mself\u001b[39m.bindings.get_collection(name, tenant, database)\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CollectionModel(\n\u001b[32m    250\u001b[39m         \u001b[38;5;28mid\u001b[39m=collection.id,\n\u001b[32m    251\u001b[39m         name=collection.name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    256\u001b[39m         database=collection.database,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mNotFoundError\u001b[39m: Collection [med] does not exists"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==== CONFIG ====\n",
    "CHROMA_PATH = \"../chroma_data\"\n",
    "COLLECTION_NAME = \"med\"\n",
    "QUERY_TEXT = \"any finance advice on dollar\"\n",
    "TOP_K = 5\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "# ==== 1. Connect to Chroma ====\n",
    "embedding_fn = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"batch_size\": 32}\n",
    ")\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "# ==== 2. Get relevant documents ====\n",
    "query_embedding = embedding_fn.embed_query(QUERY_TEXT)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"documents\"]\n",
    ")\n",
    "\n",
    "retrieved_docs = results[\"documents\"][0]\n",
    "context_text = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "# ==== 3. Send to Groq LLM ====\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a medical expert. Using the context below, answer the query clearly and concisely.\n",
    "\n",
    "Query: {QUERY_TEXT}\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "If the answer is not in the context, say \"I donâ€™t have enough information.\"\n",
    "\"\"\"\n",
    "\n",
    "chat_completion = groq_client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\", \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“„ LLM Answer:\\n\")\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650949e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 00:34:19.992 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.993 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.993 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.993 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ðŸ”¹ Load Groq API key from environment variable\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "# ðŸ”¹ ChromaDB setup\n",
    "client = chromadb.PersistentClient(path=\"./chroma_data\")\n",
    "collection_name = \"med\"\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Retrieve collection\n",
    "try:\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "except Exception:\n",
    "    st.error(f\"Collection '{collection_name}' not found.\")\n",
    "    st.stop()\n",
    "\n",
    "# ðŸ”¹ Streamlit UI\n",
    "st.set_page_config(page_title=\"Groq + Chroma Chat\", page_icon=\"ðŸ’¬\")\n",
    "st.title(\"ðŸ’¬ Chat with Your Chroma Data via Groq LLM\")\n",
    "\n",
    "# Chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages\n",
    "for msg in st.session_state.messages:\n",
    "    role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(msg[\"content\"])\n",
    "\n",
    "# Input box\n",
    "if prompt := st.chat_input(\"Ask me something...\"):\n",
    "    # Save user message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Step 1: Embed query & search Chroma\n",
    "    query_embedding = embedding_model.encode([prompt]).tolist()\n",
    "    results = collection.query(query_embeddings=query_embedding, n_results=5)\n",
    "\n",
    "    retrieved_docs = [doc for doc in results[\"documents\"][0]]\n",
    "    context_text = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Step 2: Send to Groq\n",
    "    full_prompt = f\"Answer the question using the context below.\\n\\nContext:\\n{context_text}\\n\\nQuestion: {prompt}\"\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Step 3: Display assistant message\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e6a18588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'med' deleted.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Connect to your persistent DB\n",
    "client = chromadb.PersistentClient(path=\"./chroma_data\")\n",
    "\n",
    "# Delete collection by name\n",
    "client.delete_collection(name=\"med\")\n",
    "print(\"Collection 'med' deleted.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
