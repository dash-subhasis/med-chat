{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bef9096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/subhasis/Projects/med-chat/research'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6fc70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3812b09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/subhasis/Projects/med-chat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "283f10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "27b1674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from pdf file\n",
    "def load_pdf_files(data):\n",
    "    loader= DirectoryLoader(\n",
    "        path=data,glob=\"*.pdf\",loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5648d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data = load_pdf_files(\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c69753fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extract_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0f982cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs: List[Document]) -> list[Document]:\n",
    "    minimal_docs: List[Document] = []\n",
    "\n",
    "    for doc in docs:\n",
    "        src=doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": src}\n",
    "            )\n",
    "        )\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4bfd9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_doc = filter_to_minimal_docs(extract_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6c2c78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the docs in smaller chunks\n",
    "\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "         chunk_size=500,\n",
    "         chunk_overlap=20\n",
    "    )\n",
    "    texts_chunk = text_splitter.split_documents(minimal_docs)\n",
    "    return texts_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e2bcc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_chunk = text_split(minimal_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b93072bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 32 chunks\n",
      "✅ Added batch of 3 chunks\n",
      "✅ Added documents to Chroma\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import uuid\n",
    "\n",
    "# Suppose `docs` is a List[Document] from LangChain\n",
    "doc_texts = [doc.page_content for doc in texts_chunk]\n",
    "\n",
    "# 1️⃣ Create embedding function\n",
    "embedding_fn = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"batch_size\": 32}\n",
    "    )\n",
    "\n",
    "# 2️⃣ Generate actual embeddings\n",
    "embeddings = embedding_fn.embed_documents(doc_texts)  # ✅ This returns List[List[float]]\n",
    "\n",
    "# 3️⃣ Create Chroma client\n",
    "client = chromadb.PersistentClient(path=\"../chroma_data\")\n",
    "collection_name = \"med\"\n",
    "\n",
    "# 4️⃣ Get or create collection\n",
    "existing_collections = [col.name for col in client.list_collections()]\n",
    "if collection_name in existing_collections:\n",
    "    collection = client.get_collection(collection_name)\n",
    "else:\n",
    "    collection = client.create_collection(name=collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "# === 4. Helper: Batch Processing ===\n",
    "def batch_list(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "# === 5. Embed & Insert Incrementally ===\n",
    "for batch in batch_list(doc_texts, 32):\n",
    "    ids = [str(uuid.uuid4()) for _ in batch]\n",
    "    batch_embeddings = embedding_fn.embed_documents(batch)\n",
    "\n",
    "# 6️⃣ Add documents & embeddings\n",
    "    collection.add(\n",
    "            ids=ids,\n",
    "            documents=batch,\n",
    "            embeddings=batch_embeddings\n",
    "        )\n",
    "    print(f\"✅ Added batch of {len(batch)} chunks\")\n",
    "\n",
    "print(\"✅ Added documents to Chroma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5a1020c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: abuse\n",
      "\n",
      "1. Score: 0.3593 | Text: Abuse\n",
      "Definition\n",
      "Abuse is defined as any thing that is harmful, injuri-\n",
      "ous, or offensive. Abuse als...\n",
      "2. Score: 0.3593 | Text: Abuse\n",
      "Definition\n",
      "Abuse is defined as any thing that is harmful, injuri-\n",
      "ous, or offensive. Abuse als...\n",
      "3. Score: 0.3593 | Text: Abuse\n",
      "Definition\n",
      "Abuse is defined as any thing that is harmful, injuri-\n",
      "ous, or offensive. Abuse als...\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# === CONFIG ===\n",
    "CHROMA_PATH = \"../chroma_data\"\n",
    "COLLECTION_NAME = \"med\"\n",
    "QUERY_TEXT = \"abuse\"  # Your search query\n",
    "TOP_K = 3  # Number of results to return\n",
    "\n",
    "# === 1. Load embedding function (must match what you used during insert) ===\n",
    "embedding_fn = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"batch_size\": 32}\n",
    "    )\n",
    "\n",
    "# === 2. Connect to Chroma and get collection ===\n",
    "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "# === 3. Embed the query ===\n",
    "query_embedding = embedding_fn.embed_query(QUERY_TEXT)\n",
    "\n",
    "# === 4. Search in Chroma ===\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"documents\", \"distances\"]\n",
    ")\n",
    "\n",
    "# === 5. Show results ===\n",
    "print(f\"\\n🔍 Query: {QUERY_TEXT}\\n\")\n",
    "for i, (doc, distance) in enumerate(zip(results[\"documents\"][0], results[\"distances\"][0]), start=1):\n",
    "    print(f\"{i}. Score: {distance:.4f} | Text: {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555067d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Collection [med] does not exists",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[112]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     15\u001b[39m embedding_fn = HuggingFaceEmbeddings(\n\u001b[32m     16\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     encode_kwargs={\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m32\u001b[39m}\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m client = chromadb.PersistentClient(path=CHROMA_PATH)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m collection = client.get_collection(COLLECTION_NAME)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ==== 2. Get relevant documents ====\u001b[39;00m\n\u001b[32m     24\u001b[39m query_embedding = embedding_fn.embed_query(QUERY_TEXT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medbot/lib/python3.11/site-packages/chromadb/api/client.py:192\u001b[39m, in \u001b[36mClient.get_collection\u001b[39m\u001b[34m(self, name, embedding_function, data_loader)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(\n\u001b[32m    185\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    190\u001b[39m     data_loader: Optional[DataLoader[Loadable]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    191\u001b[39m ) -> Collection:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._server.get_collection(\n\u001b[32m    193\u001b[39m         name=name,\n\u001b[32m    194\u001b[39m         tenant=\u001b[38;5;28mself\u001b[39m.tenant,\n\u001b[32m    195\u001b[39m         database=\u001b[38;5;28mself\u001b[39m.database,\n\u001b[32m    196\u001b[39m     )\n\u001b[32m    197\u001b[39m     persisted_ef_config = model.configuration_json.get(\u001b[33m\"\u001b[39m\u001b[33membedding_function\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    199\u001b[39m     validate_embedding_function_conflict_on_get(\n\u001b[32m    200\u001b[39m         embedding_function, persisted_ef_config\n\u001b[32m    201\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medbot/lib/python3.11/site-packages/chromadb/api/rust.py:248\u001b[39m, in \u001b[36mRustBindingsAPI.get_collection\u001b[39m\u001b[34m(self, name, tenant, database)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(\n\u001b[32m    243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    246\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    247\u001b[39m ) -> CollectionModel:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     collection = \u001b[38;5;28mself\u001b[39m.bindings.get_collection(name, tenant, database)\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CollectionModel(\n\u001b[32m    250\u001b[39m         \u001b[38;5;28mid\u001b[39m=collection.id,\n\u001b[32m    251\u001b[39m         name=collection.name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    256\u001b[39m         database=collection.database,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mNotFoundError\u001b[39m: Collection [med] does not exists"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==== CONFIG ====\n",
    "CHROMA_PATH = \"../chroma_data\"\n",
    "COLLECTION_NAME = \"med\"\n",
    "QUERY_TEXT = \"any finance advice on dollar\"\n",
    "TOP_K = 5\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "# ==== 1. Connect to Chroma ====\n",
    "embedding_fn = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"batch_size\": 32}\n",
    ")\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "# ==== 2. Get relevant documents ====\n",
    "query_embedding = embedding_fn.embed_query(QUERY_TEXT)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"documents\"]\n",
    ")\n",
    "\n",
    "retrieved_docs = results[\"documents\"][0]\n",
    "context_text = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "# ==== 3. Send to Groq LLM ====\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a medical expert. Using the context below, answer the query clearly and concisely.\n",
    "\n",
    "Query: {QUERY_TEXT}\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "If the answer is not in the context, say \"I don’t have enough information.\"\n",
    "\"\"\"\n",
    "\n",
    "chat_completion = groq_client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\", \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"\\n📄 LLM Answer:\\n\")\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650949e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 00:34:19.992 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.993 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.993 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.993 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-16 00:34:19.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 🔹 Load Groq API key from environment variable\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "# 🔹 ChromaDB setup\n",
    "client = chromadb.PersistentClient(path=\"./chroma_data\")\n",
    "collection_name = \"med\"\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Retrieve collection\n",
    "try:\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "except Exception:\n",
    "    st.error(f\"Collection '{collection_name}' not found.\")\n",
    "    st.stop()\n",
    "\n",
    "# 🔹 Streamlit UI\n",
    "st.set_page_config(page_title=\"Groq + Chroma Chat\", page_icon=\"💬\")\n",
    "st.title(\"💬 Chat with Your Chroma Data via Groq LLM\")\n",
    "\n",
    "# Chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages\n",
    "for msg in st.session_state.messages:\n",
    "    role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(msg[\"content\"])\n",
    "\n",
    "# Input box\n",
    "if prompt := st.chat_input(\"Ask me something...\"):\n",
    "    # Save user message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Step 1: Embed query & search Chroma\n",
    "    query_embedding = embedding_model.encode([prompt]).tolist()\n",
    "    results = collection.query(query_embeddings=query_embedding, n_results=5)\n",
    "\n",
    "    retrieved_docs = [doc for doc in results[\"documents\"][0]]\n",
    "    context_text = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Step 2: Send to Groq\n",
    "    full_prompt = f\"Answer the question using the context below.\\n\\nContext:\\n{context_text}\\n\\nQuestion: {prompt}\"\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Step 3: Display assistant message\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e6a18588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'med' deleted.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Connect to your persistent DB\n",
    "client = chromadb.PersistentClient(path=\"./chroma_data\")\n",
    "\n",
    "# Delete collection by name\n",
    "client.delete_collection(name=\"med\")\n",
    "print(\"Collection 'med' deleted.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
